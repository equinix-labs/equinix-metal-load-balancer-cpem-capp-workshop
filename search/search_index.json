{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Introduction","text":""},{"location":"#about-equinix-labs","title":"About Equinix Labs","text":"<p>Equinix Labs offers workshops, proof of concepts, and tools for exploring and bootstrapping Equinix digital infrastructure including Fabric, Metal, and Network Edge.</p>"},{"location":"#about-the-workshop","title":"About the workshop","text":"<p>The goals of this workshop are:</p> <ul> <li>Become familiar with Equinix Metal Load Balancers inside our Kubernetes integrations</li> <li>Provision a Cluster API based cluster using bare metal servers with an Equinix Metal Load Balancer in front of its control plane nodes.</li> <li>Configure Cloud Provider Equinix Metal to be able to deploy serviced based load balancers inside your kubernetes cluster.</li> <li>Deploy a simple NGINX service and expose it using the Equinix Metal Load Balancer.</li> </ul>"},{"location":"#workshop-agenda","title":"Workshop agenda","text":"<p>This workshop is split into four parts:</p> Part Title Duration 1 Account and API Key Setup 10 minutes 2 LBaaS with CAPP 30 minutes 3 LBaaS with CPEM 20 minutes 4 Conclusion 2 minutes"},{"location":"parts/1-account-api-key/","title":"Part 1: Account and API Key Setup","text":"<p>To run this workshop you will need access to an Equinix Metal Account or create a new one following step 1 below.</p> <p>Note: You are responsible for the cost of resources created in your Equinix Metal account while running this workshop.</p>"},{"location":"parts/1-account-api-key/#pre-requisites","title":"Pre-requisites","text":"<p>The following tools will be needed on your local development environment where you will be running most of the commands in this guide:</p> <ul> <li>A Unix-like environment (Linux, MacOS, Windows WSL)</li> <li>docker</li> <li>kind</li> <li>helm</li> <li>clusterctl</li> <li>kubectl</li> </ul>"},{"location":"parts/1-account-api-key/#steps","title":"Steps","text":""},{"location":"parts/1-account-api-key/#1-create-an-equinix-metal-account","title":"1. Create an Equinix Metal account","text":"<p>If you have never used Equinix Metal before, don't worry, you just need 2 minutes to sign-up and create your first organization. If you have any doubt you can watch our Getting Started with Equinix Metal video.</p>"},{"location":"parts/1-account-api-key/#2-create-an-api-key","title":"2. Create an API key","text":"<p>API keys in Metal can be tied to your user or to a single project. For this workshop we will need a user-level API key.</p> <p>Note: Project API keys do not have access to the entirety of the API; some endpoints can only be used by personal API keys.</p> <p>To create a new user API key, access your user Profile in the Equinix Metal console, click on the User Icon, and click My Profile.</p> <p></p> <ul> <li>Select the <code>API Keys</code> tab.</li> <li>Click on <code>+ Add New Key</code>.</li> <li>Create a new key with <code>Read/Write</code> permissions.</li> </ul> <p></p> <p>Copy the API key and save it in a safe place. You will need it later.</p>"},{"location":"parts/1-account-api-key/#3-get-your-project-id","title":"3. Get your Project ID","text":"<p>To get your project ID, click on the <code>Projects</code> tab in the Equinix Metal console. You will see a list of projects you have access to. Click on the project you want to use for this workshop. Click on the <code>Project Settings</code> tab and you will see the project ID.</p> <p></p> <p>Copy the project ID and save it in a safe place. You will need it later.</p>"},{"location":"parts/1-account-api-key/#4-create-an-ssh-key","title":"4. Create an SSH key","text":"<p>Follow the documentation for creating and adding an SSH key to your account here.</p> <p>We'll use this key to access the machines we create in the workshop and Cluster API will need it as part of its configuration.</p> <p>Copy the public SSH key and save it in a safe place. You will need it later.</p>"},{"location":"parts/1-account-api-key/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>What is the purpose of an API key?</li> <li>What is the difference between a user and project API key?</li> <li>Why do we need to install these tools?</li> <li>What is the purpose of each tool?</li> </ul>"},{"location":"parts/2-deploy-cluster/","title":"2. Deploy Cluster","text":""},{"location":"parts/2-deploy-cluster/#part-2-deploy-cluster","title":"Part 2: Deploy Cluster","text":""},{"location":"parts/2-deploy-cluster/#steps","title":"Steps","text":""},{"location":"parts/2-deploy-cluster/#1-setup-work-environment","title":"1. Setup Work environment","text":"<p>We're going to walk through setting up the required tools in a Unix-like environment. This will be the environment where you will be running most of the commands in this guide. This could be a Linux host you deploy on Equinix Metal, a Mac, or a Windows machine with WSL. For this sake of this guide, we're going to presume a freshly deployed Ubuntu host.</p>"},{"location":"parts/2-deploy-cluster/#install-docker","title":"Install Docker","text":"<p>First, we need to install Docker. This is the container runtime that we will use to run our management Kubernetes cluster. The easiest way to do this is with the docker convenience script.</p> Bash<pre><code>curl -fsSL https://get.docker.com -o get-docker.sh\nsudo sh ./get-docker.sh\n</code></pre>"},{"location":"parts/2-deploy-cluster/#install-kind-kubernetes-in-docker","title":"Install kind (Kubernetes in Docker)","text":"<p>Next, we install Kind. Kind is a tool for running local Kubernetes clusters using Docker container \"nodes\". We'll use this to create a local kubernetes cluster that we can install Cluster API Provider Packet into and then create a Kubernetes cluster on Equinix Metal.</p> <p>Please replace the version number below with the latest number from https://github.com/kubernetes-sigs/kind/releases.</p> Bash<pre><code># For AMD64 / x86_64\ncurl -L https://kind.sigs.k8s.io/dl/v0.23.0/kind-linux-amd64 -o ./kind\nsudo install -o root -g root -m 0755 ./kind /usr/local/bin/kind\n</code></pre>"},{"location":"parts/2-deploy-cluster/#install-clusterctl","title":"Install clusterctl","text":"<p>Clusterctl is the CLI tool that handles installing and managing Cluster API based management clusters. We'll use this to install Cluster API Provider Packet into our local kubernetes cluster.</p> <p>Please replace the version number below with the latest number from https://github.com/kubernetes-sigs/cluster-api/releases.</p> Bash<pre><code>curl -L https://github.com/kubernetes-sigs/cluster-api/releases/download/v1.7.4/clusterctl-linux-amd64 -o ./clusterctl\nsudo install -o root -g root -m 0755 ./clusterctl /usr/local/bin/clusterctl\n</code></pre>"},{"location":"parts/2-deploy-cluster/#install-kubectl","title":"Install kubectl","text":"<p>We use kubectl to interact with our kubernetes cluster.</p> Bash<pre><code>curl -LO \"https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl\"\nsudo install -o root -g root -m 0755 ./kubectl /usr/local/bin/kubectl\n</code></pre>"},{"location":"parts/2-deploy-cluster/#install-helm","title":"Install helm","text":"<p>We'll use helm to easily install applications into our kubernetes cluster, we won't need it till the last section.</p> Bash<pre><code>curl -fsSL -o get_helm.sh https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3\nsudo bash get_helm.sh\n</code></pre>"},{"location":"parts/2-deploy-cluster/#2-deploy-a-local-kubernetes-cluster","title":"2. Deploy a local Kubernetes cluster","text":"<p>Now that we have all the tools we need, let's deploy a local Kubernetes cluster using kind.</p> Bash<pre><code>kind create cluster --name capp\n</code></pre> <p>The output should look like the following:</p> Bash Session<pre><code>Creating cluster \"capp\" ...\n\u2713 Ensuring node image (kindest/node:v1.30.0) \ud83d\uddbc\n\u2713 Preparing nodes \ud83d\udce6\n\u2713 Writing configuration \ud83d\udcdc\n\u2713 Starting control-plane \ud83d\udd79\ufe0f\n\u2713 Installing CNI \ud83d\udd0c\n\u2713 Installing StorageClass \ud83d\udcbe\nSet kubectl context to \"kind-capp\"\nYou can now use your cluster with:\n\nkubectl cluster-info --context kind-capp\n\nThanks for using kind! \ud83d\ude0a\n</code></pre> <p>If you're on a freshly installed Linux box like we are, you're kubectl should default to accessing the cluster you just created. You can verify this by running:</p> Bash<pre><code>kubectl get nodes\n</code></pre> <p>Which will have the following output</p> Bash Session<pre><code>NAME                 STATUS     ROLES           AGE   VERSION\ncapp-control-plane   NotReady   control-plane   15s   v1.30.0\n</code></pre> <p>If you're in another environment you may need to do the following to setup your kubectl to use the kind cluster:</p> Bash<pre><code>kind get kubeconfig --name=capp &gt; kubeconfig-kind\nexport KUBECONFIG=kubeconfig-kind\n</code></pre>"},{"location":"parts/2-deploy-cluster/#3-install-cluster-api-provider-packet","title":"3. Install Cluster API Provider Packet","text":"<p>Now that we have a Kubernetes cluster running, we can install Cluster API Provider Packet into it. We need to set some environment variables first, in order to properly configure the provider.</p> <p>We've included some default values below for the environment variables, but you should replace them with your own values. You'll also need your API key and project ID from the Equinix Metal portal. You will need to use a User API key.</p> Bash<pre><code>export CONTROLPLANE_NODE_TYPE=m3.small.x86\nexport WORKER_NODE_TYPE=m3.small.x86\nexport KUBERNETES_VERSION=1.30.2\nexport METRO=da\nexport PACKET_API_KEY=&lt;YOUR_API_KEY&gt;\nexport PROJECT_ID=&lt;YOUR_PROJECT_ID&gt;\nexport SSH_KEY=&lt;YOUR_PUBLIC_SSH_KEY_CONTENTS&gt; # for example $(&lt;$HOME/.ssh/id_rsa.pub)\n</code></pre> <p>Now we can install Cluster API Provider Packet into our local Kubernetes cluster.</p> Bash<pre><code>clusterctl init --infrastructure=packet\n</code></pre> <p>The output looks something like the following:</p> Bash Session<pre><code>Fetching providers\nInstalling cert-manager Version=\"v1.15.1\"\nWaiting for cert-manager to be available...\nInstalling Provider=\"cluster-api\" Version=\"v1.7.4\" TargetNamespace=\"capi-system\"\nInstalling Provider=\"bootstrap-kubeadm\" Version=\"v1.7.4\" TargetNamespace=\"capi-kubeadm-bootstrap-system\"\nInstalling Provider=\"control-plane-kubeadm\" Version=\"v1.7.4\" TargetNamespace=\"capi-kubeadm-control-plane-system\"\nInstalling Provider=\"infrastructure-packet\" Version=\"v0.9.0\" TargetNamespace=\"cluster-api-provider-packet-system\"\n\nYour management cluster has been initialized successfully!\n\nYou can now create your first workload cluster by running the following:\n\n  clusterctl generate cluster [name] --kubernetes-version [version] | kubectl apply -f -\n</code></pre>"},{"location":"parts/2-deploy-cluster/#4-deploy-workload-cluster-with-capp","title":"4. Deploy workload cluster with CAPP","text":"<p>Now that we have Cluster API Provider Packet installed, we can deploy a Kubernetes cluster on Equinix Metal. We'll use the <code>clusterctl generate cluster</code> command to generate a cluster manifest, and then apply it to our management cluster.</p> <p>The following command creates the cluster with three control plane nodes and two worker nodes in order to be able to demonstrate the load balancer functionality of the cluster.</p> Bash<pre><code>clusterctl generate cluster my-lbaas-demo --flavor emlb --control-plane-machine-count 3 --worker-machine-count 2 &gt; my-lbaas-demo.yaml\nkubectl apply -f my-lbaas-demo.yaml\n</code></pre> <p>:watch: It should take around 20 minutes for the cluster to be created.</p>"},{"location":"parts/2-deploy-cluster/#5-verify-workload-cluster","title":"5. Verify workload cluster","text":"<p>Let's validate that the cluster was created successfully.</p> <p>First check the status of the cluster and ensure it says <code>Provisioned</code> in the PHASE column.</p> Bash<pre><code>$ kubectl get cluster\nNAME            CLUSTERCLASS   PHASE         AGE   VERSION\nmy-lbaas-demo                  Provisioned   19m\n</code></pre> <p>Next, check the status of the machines in the cluster. You should see the control plane and worker say <code>Running</code> in the PHASE column. Make sure you have the right count of control plane and worker nodes, for this example you should have 3 control plane nodes and 2 worker nodes.</p> Bash<pre><code>$ kubectl get machines\nNAME                                 CLUSTER         NODENAME                             PROVIDERID                                            PHASE     AGE   VERSION\nmy-lbaas-demo-control-plane-br5xj    my-lbaas-demo   my-lbaas-demo-control-plane-br5xj    equinixmetal://f23a7719-57fb-48ff-81a7-564d36b1b869   Running   10m   v1.30.2\nmy-lbaas-demo-control-plane-mmm5j    my-lbaas-demo   my-lbaas-demo-control-plane-mmm5j    equinixmetal://8693714a-0e9d-4d9c-81e1-5c4fdaec7a43   Running   20m   v1.30.2\nmy-lbaas-demo-control-plane-xmm8z    my-lbaas-demo   my-lbaas-demo-control-plane-xmm8z    equinixmetal://9ee9e770-fac2-4041-bde0-c54597daff20   Running   15m   v1.30.2\nmy-lbaas-demo-worker-a-tmjck-bggv9   my-lbaas-demo   my-lbaas-demo-worker-a-tmjck-bggv9   equinixmetal://05cae0b9-508e-49f1-a8a3-bddc76c6ebd1   Running   20m   v1.30.2\nmy-lbaas-demo-worker-a-tmjck-nr5zk   my-lbaas-demo   my-lbaas-demo-worker-a-tmjck-nr5zk   equinixmetal://b720948a-a3ac-4187-8051-0687c6cb1fa2   Running   20m   v1.30.2\n</code></pre>"},{"location":"parts/2-deploy-cluster/#6-access-the-workload-cluster","title":"6. Access the workload cluster","text":"<p>Now that the cluster is up and running, we can access it using kubectl.</p> <p>First we need to get the kubeconfig for the cluster. We can do this by running the following command:</p> Bash<pre><code>clusterctl get kubeconfig my-lbaas-demo &gt; kubeconfig\nexport KUBECONFIG=kubeconfig\n</code></pre> <p>Now we can run kubectl commands against the cluster. For example, to get the nodes in the cluster:</p> Bash<pre><code>$ kubectl get nodes\nNAME                                 STATUS     ROLES           AGE   VERSION\nmy-lbaas-demo-control-plane-br5xj    NotReady   control-plane   11m   v1.30.2\nmy-lbaas-demo-control-plane-mmm5j    NotReady   control-plane   24m   v1.30.2\nmy-lbaas-demo-control-plane-xmm8z    NotReady   control-plane   19m   v1.30.2\nmy-lbaas-demo-worker-a-tmjck-bggv9   NotReady   &lt;none&gt;          16m   v1.30.2\nmy-lbaas-demo-worker-a-tmjck-nr5zk   NotReady   &lt;none&gt;          20m   v1.30.2\n</code></pre> <p>You'll note the nodes say they're <code>NotReady</code>. This is because we haven't installed a CNI plugin yet. We'll do that in the next section.</p>"},{"location":"parts/2-deploy-cluster/#7-install-a-cni-plugin","title":"7. Install a CNI plugin","text":"<p>We need to install a CNI plugin in order to get networking working in our cluster. We'll use Calico for this example. We'll be using helm to do the install, but you can visit the Calico website for other installation methods like operator or manifest based.</p> Bash<pre><code>helm repo add projectcalico https://docs.tigera.io/calico/charts\nhelm install calico projectcalico/tigera-operator --namespace tigera-operator --create-namespace\n</code></pre> <p>Wait about 30 seconds for the calico pods to start up, then run the following command to check that they're all running.</p> Bash<pre><code>$ kubectl get pods -n calico-system\nNAME                                       READY   STATUS    RESTARTS   AGE\ncalico-kube-controllers-85cd86bb8b-hjd6n   1/1     Running   0          63s\ncalico-node-474qq                          1/1     Running   0          63s\ncalico-node-dbqsp                          1/1     Running   0          63s\ncalico-node-pd9c2                          1/1     Running   0          63s\ncalico-node-rr5vg                          1/1     Running   0          63s\ncalico-node-vm5fk                          1/1     Running   0          63s\ncalico-typha-7df687ff48-mmn44              1/1     Running   0          54s\ncalico-typha-7df687ff48-r5ftw              1/1     Running   0          63s\ncalico-typha-7df687ff48-wwd8b              1/1     Running   0          54s\ncsi-node-driver-fp5h5                      2/2     Running   0          63s\ncsi-node-driver-lhddd                      2/2     Running   0          63s\ncsi-node-driver-rrvfs                      2/2     Running   0          63s\ncsi-node-driver-wmcn8                      2/2     Running   0          63s\ncsi-node-driver-x6ptt                      2/2     Running   0          63s\n</code></pre> <p>Now you can check the nodes again and they should be <code>Ready</code>.</p> Bash<pre><code>$ kubectl get nodes\nNAME                                 STATUS   ROLES           AGE   VERSION\nmy-lbaas-demo-control-plane-br5xj    Ready    control-plane   22m   v1.30.2\nmy-lbaas-demo-control-plane-mmm5j    Ready    control-plane   34m   v1.30.2\nmy-lbaas-demo-control-plane-xmm8z    Ready    control-plane   29m   v1.30.2\nmy-lbaas-demo-worker-a-tmjck-bggv9   Ready    &lt;none&gt;          27m   v1.30.2\nmy-lbaas-demo-worker-a-tmjck-nr5zk   Ready    &lt;none&gt;          30m   v1.30.2\n</code></pre>"},{"location":"parts/2-deploy-cluster/#8-were-done","title":"8. We're done","text":"<p>We've successfully deployed a Kubernetes cluster with a highly available control plane on Equinix Metal using Cluster API Provider Packet. Your cluster is now up and running and ready for you to deploy your applications to it. Which we'll do in the next step when we configure the cluster for service based load balancing.</p> <p>For more information on what it means to have a highly available control plane, see the Kubernetes documentation about a stacked etcd topology, which is what we've deployed here.</p>"},{"location":"parts/2-deploy-cluster/#common-issues-that-may-arise","title":"Common issues that may arise","text":"<ul> <li>No more devices of a specific plan available in the metro you deployed the cluster to.</li> <li>Try deploying to a different metro.</li> <li>Try deploying a different plan.</li> </ul>"},{"location":"parts/2-deploy-cluster/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>Why do we need to install these tools?</li> <li>Are there other ways to do the installation of these items?</li> <li>Can I use another kubernetes deployment method to create the management cluster?</li> </ul>"},{"location":"parts/3-deploy-app/","title":"3. Deploy App","text":""},{"location":"parts/3-deploy-app/#part-3-deploy-application","title":"Part 3: Deploy Application","text":""},{"location":"parts/3-deploy-app/#steps","title":"Steps","text":""},{"location":"parts/3-deploy-app/#1-configure-cpem","title":"1. Configure CPEM","text":"<p>As part of creating our Kubernetes cluster in the last section, Cluster API Provider Packet installed Cloud Provider Equinix Metal for us. We will now configure it to set up service load balancers for us.</p> <ol> <li> <p>Delete current secret file</p> Bash<pre><code>kubectl delete secrets -n kube-system metal-cloud-config\n</code></pre> </li> <li> <p>Create a new secret file</p> <p>For example, create a file named <code>cloud-sa.json</code> with the following content:`</p> Text Only<pre><code>{\n\"apiKey\": \"&lt;YOUR_API_KEY&gt;\",\n\"projectID\": \"&lt;YOUR_PROJECT_ID&gt;\",\n\"metro\": \"&lt;YOUR_METRO&gt;\",\n\"loadbalancer\": \"emlb:///&lt;YOUR_METRO&gt;\"\n}\n</code></pre> <p>It might look like the following for our example in the Dallas metro:</p> Bash<pre><code>&gt; cat cloud-sa.json\n{\n\"apiKey\": \"1234567890abcdefghijklmnopqrstuv\",\n\"projectID\": \"1234abcd-1234-abcd-ab12-abcdefgh1234\",\n\"metro\": \"da\",\n\"loadbalancer\": \"emlb:///da\"\n}\n</code></pre> </li> <li> <p>Apply the new secret file</p> Bash<pre><code>kubectl create secret generic --from-file=./cloud-sa.json metal-cloud-config -n kube-system\n</code></pre> </li> <li> <p>Restart the cloud provider</p> Bash<pre><code>kubectl rollout restart ds -n kube-system cloud-provider-equinix-metal\n</code></pre> </li> <li> <p>Make sure Cloud Provider Equinix Metal has come back up, it should look like the below. If it doesn't say Running in the STATUS column, that could indicate a problem in your cloud-sa.json file you created earlier or that you forgot to <code>kubectl create</code> the secret using that file.</p> Bash<pre><code>kubectl get pods -n kube-system -l app=cloud-provider-equinix-metal\nNAME                                 READY   STATUS    RESTARTS   AGE\ncloud-provider-equinix-metal-6l7zh   1/1     Running   0          60s\ncloud-provider-equinix-metal-8dzpq   1/1     Running   0          62s\ncloud-provider-equinix-metal-dw89f   1/1     Running   0          63s\n</code></pre> </li> </ol>"},{"location":"parts/3-deploy-app/#2-deploy-nginx","title":"2. Deploy NGINX","text":"<ol> <li> <p>Deploy a sample application</p> <p>Here we'll deploy a sample application to test the load balancer. We'll use the nginx deployment example from the Kubernetes documentation. This will set up a website that we can access through the load balancer.</p> Bash<pre><code>kubectl create namespace lbaas-demo\nkubectl apply -f https://k8s.io/examples/application/deployment.yaml -n lbaas-demo\nkubectl -n lbaas-demo -l app=nginx get pods\n</code></pre> </li> <li> <p>Expose the aplication</p> Bash<pre><code>kubectl expose --namespace lbaas-demo deployment nginx-deployment --type LoadBalancer --port 80\n</code></pre> </li> <li> <p>Wait for the load balancer to be created. This may take a few minutes.</p> <p>Keep running the following command and watch for the External IP address column to have a value and not say Pending...</p> Bash<pre><code>kubectl get svc nginx-deployment -n lbaas-demo\nNAME               TYPE           CLUSTER-IP      EXTERNAL-IP    PORT(S)        AGE\nnginx-deployment   LoadBalancer   172.26.58.144   &lt;pending&gt;      80:30486/TCP   19s\n</code></pre> </li> <li> <p>You should now be able to visit the IP address listed in the EXTERNAL_IP column in your browser and see the default nginx page. </p> </li> <li> <p>Investigate the load balancer and its origin pool on the portal.</p> </li> <li> <p>Go to the Equinix Metal portal and navigate to the project you are using.</p> </li> <li> <p>Click on the Load Balancers tab. Your new load balancer should be listed and start with \"usage=cloud-provider\" it should look like below. </p> </li> <li> <p>Click on that load balancer and go to \"Listener Ports\" and you should see the listener port that was created, like below. </p> </li> <li> <p>Click on the \"Manage Pool\" and it'll take you to the origin pool for the Load Balancer, which should look similar to below: </p> </li> </ol>"},{"location":"parts/3-deploy-app/#discussion","title":"Discussion","text":"<p>Before proceeding to the next part let's take a few minutes to discuss what we did. Here are some questions to start the discussion.</p> <ul> <li>What is Cloud Provider Equinix Metal?</li> <li>What is the purpose of the cloud-sa.json file?</li> </ul>"},{"location":"parts/conclusion/","title":"Conclusion","text":""},{"location":"parts/conclusion/#part-4-conclusion","title":"Part 4: Conclusion","text":"<p>Thank you for participating in the workshop! Let's recap some of the key takeways that we've learned:</p> <ul> <li>We learned how to create a Kubernetes cluster using Cluster API Provider Packet.</li> <li>We learned how to configure Cloud Provider Equinix Metal to set up service load balancers for us.</li> <li>We learned how to deploy a sample application to our Kubernetes cluster.</li> </ul>"},{"location":"parts/conclusion/#next-steps","title":"Next Steps","text":"<ul> <li>You may wish to delete your kubernetes cluster to avoid incurring any costs. You can do this by running the following command:</li> </ul> Bash<pre><code>export KUBECONFIG=kubeconfig-kind\nkubectl delete cluster my-lbaas-demo\n</code></pre> <p>Then verify that the machines are deleted from your project.</p> <ul> <li>You can also delete the entire project. This will delete all resources in the project, including the machines and the project itself.</li> </ul>"},{"location":"parts/conclusion/#resources","title":"Resources","text":"<p>Here are a few other resources to look at to continue your Equinix Metal journey:</p> <ul> <li>Deploy @ Equinix: A one-stop shop for blogs, guides, and plenty of other resources.</li> <li>Equinix Metal Docs: Equinix Metal official documentation.</li> <li>Equinix Metal APIs: Programmatically interact with Equinix Metal</li> <li>Equinix Labs: Provides SDKs and Terraform modultes for Infrastructure as Code tools.</li> <li>Equinix Community: A global community for customers and Equinix users.</li> </ul>"}]}